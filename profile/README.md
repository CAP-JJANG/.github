 ![header](https://capsule-render.vercel.app/api?type=waving&color=gradient&height=300&section=header&text=CAPJJANG&fontSize=90&fontAlignY=40&desc=2023%20공개%20SW%20개발자%20대회&descAlign=70)

<br><br>
<div align="center">
  
[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FCap-JJANG&count_bg=%23000000&title_bg=%23000000&icon=github.svg&icon_color=%23FFFFFF&title=GitHub&edge_flat=false)](https://hits.seeyoufarm.com)
  
</div>
<br><br>

## :fire: 제목
**[ENG]**  
Handwritten Acoustic Signal Recognition Technology Using Deep Learning (CSD-Model)

<br>

**[KOR]**  
Deep Learning을 이용한 손글씨 음향 신호 인식 기술 (CSD-Model)

<br><br>
## :raised_hands: 소개
**[ENG]**   
Hello, This is Capjjang, participating in the 2023 Open SW Developer Contest.

When using a small wearable device in everyday life, the small keyboard panel sometimes makes the input uncomfortable. To address this inconvenience, the team wants to make the table a single panel. Instead of small screens, we have developed a Capjang Spectrogram Detection-Model (CSD-Model/Character Recognition Model through Sound Signal Detection) to enable the device to recognize the handwriting through the sound of writing on a wide table. 

  About 50 people recorded unique acoustic signals generated by writing alphabets on tables with Android voice recording, collecting about 900 acoustic data per class. We found a peak point where an explosive signal suddenly occurred in the collected dataset and implemented a function that cuts the acoustic file from the peak by one second. The sound file cut into 1 second was saved as a wave file and converted into a spectrogram, an image in the form of a time-frequency graph. 

  To augment the dataset, we converted the wave file into a spectrogram image after performing 1.25x and 1.50x speed to create x3x images of the original data. In addition, random masking was performed once, horizontally and vertically from the original spectrogram to create x3x images of the original data, augmenting the dataset to about 4500 per class. These images were converted into tensors and normalized to perform data preprocessing. We loaded CNN's Resnet-34 model to perform K-fold cross-validation, repeatedly learn the model for each fold, and then store the most accurate model (CSD-Model).

  The instrument records a handwritten acoustic signal every 1.5 seconds, stores the acoustic signal in a ByteRay object and sends the object to the server via RestAPI as a POST request. The server converts the Byte Array object received from the Client into a wave file and converts it to a spectrogram image. Apply the converted image to the CSD-Model to extract the result value and send a POST response to the Client.

  This technology can be used in a variety of fields, such as the development of the watch app "WRITE NOW" and the mobile app "RIGHT NOW." "WRITE NOW" is an application for watches that can recognize handwriting written on a desk and display it on the screen. Write "SOS" on your desk and press the check button to make an emergency call. You can also copy the written text and use it like a keyboard, and send a message. "RIGHT NOW" is a learning mobile application that recognizes children's handwritten sounds and scores English words.

<br>

**[KOR]**  
안녕하세요. 2023 공개 SW 개발자 대회에 참가한 '캡짱'입니다.

일상 속에서 소형 웨어러블 기기 사용시, 작은 키보드 패널로 인해 입력에 불편함을 느낄 때가 있습니다. 본 팀은 이러한 불편함을 해소하기 위해 테이블을 하나의 패널로 만들고자 합니다. 작은 화면 대신, 넓은 테이블에서 글씨 쓰는 음향을 통해 기기에서 그 글씨를 인식할 수 있도록 CSD-Model(Capjjang Spectrogram Detection-Model/음향 신호 감지를 통한 문자 인식 모델)을 개발했습니다. 

  약 50명의 사람들을 통해 Android 음성녹음 기능으로 테이블 위에서 알파벳(소문자)을 썼을 때 생기는 고유한 음향 신호를 녹음하여, 클래스당 약 900개의 음향 데이터를 수집했습니다. 수집한 데이터셋에서 갑자기 폭발적인 신호가 발생하는 Peak 지점을 찾아 Peak 부터 음향파일을 1초를 자르는 함수를 구현했습니다. 1초로 잘라진 음향파일을 wav 파일로 저장하고, 시간-주파수 그래프 형태의 이미지인 스펙트로그램으로 변환했습니다. 

  데이터셋 증강을 위해, wav 파일을 1.25배속, 1.50 배속을 진행한 후 스펙트로그램 이미지로 변환하여 원본 데이터의 x3배 이미지를 생성했습니다. 뿐만 아니라, 원본 스펙트로그램에서 가로, 세로 한 번씩 랜덤 마스킹을 진행하여 원본데이터의 x3배 이미지를 생성하여 클래스당 약 4500개로 데이터셋을 증강했습니다. 이 이미지들을 tensor로 변환하고, 정규화를 진행하여 데이터 전처리를 진행했습니다. CNN의 Resnet-34 모델을 로드하여 K-fold 교차 검증을 진행하고, 각 폴드에 대해 반복적으로 모델을 학습한 후 최고 정확도를 가진 모델(CSD-Model)을 저장하도록 했습니다.

  기기를 이용해 1.5초마다 손글씨 음향 신호를 녹음하여 ByteArray 객체에 해당 음향신호를 저장하여 RestAPI를 통해 Server로 해당 객체를 POST 요청으로 보냅니다. Server에서는 Client로부터 받은 ByteArray객체를 wav 파일로 변환한 뒤, 스펙트로그램 이미지로 변환합니다. 변환된 이미지를 CSD-Model에 적용시켜 결과값을 추출하여 Client에게 POST 응답을 보냅니다.

  이 기술은 다양한 분야에서 활용될 수 있으며, 그 예시로 워치용 어플 “WriteNow”와 모바일 어플 “RightNow”를 개발했습니다. “WriteNow”는 워치용 어플로 책상에서 쓴 손글씨를 인식하여 화면에 나타낼 수 있습니다. “sos”라고 책상에 쓰고 체크 버튼을 누르면 긴급 전화를 걸 수 있습니다. 또한, 써진 글씨를 복사하여 키보드처럼 사용할 수 있으며, 메시지도 보낼 수 있습니다. “RightNow는 아이들의 손글씨 음향을 인식하여 영단어를 맞게 썼는지 채점하는 학습용 모바일 어플입니다.


<br><br>
## ✨ 기대 효과
**[ENG]**
1. Various applications: It can be used in a variety of fields, including improved user interface based on letter input, handwriting recognition system, and handwriting-based automatic translation and recognition technology.
2. Convenient input method: Instead of a keyboard or touch screen, you can convert it into an acoustic signal and input it by handwriting.
3. Improve accessibility: Improve access to information for users with voice or motion difficulties. Blind people or people with physical constraints can also recognize handwritten information by converting it into sound, allowing various users to freely access the information.

<br>

**[KOR]**
1. 다양한 응용 분야: 글자 입력 기반의 사용자 인터페이스 개선, 필기인식 시스템, 손글씨 기반의 자동번역 및 인식기술 등 다양한 분야에서 활용될 수 있습니다.
2. 편리한 입력 방식 제공: 키보드나 터치스크린 대신 손으로 직접 글자를 쓰는 동작을 통해 음향신호로 변환하여 입력할 수 있습니다.
3. 접근성 개선: 음성이나 동작에 어려움이 있는 사용자들에게 정보 접근성을 개선해줍니다. 시각 장애인이나 신체적 제약이 있는 사람들도 손글씨로 표현된 정보를 소리로 변환하여 인식할 수 있어, 다양한 사용자들이 정보에 자유롭게 접근할 수 있습니다.


<br><br>
## 💪 주요 기능
### ✔️ MakeDataset
Details in [MakeDataset Repository](https://github.com/CAP-JJANG/MakeDataset)  

### ✔️ CSD-Model
**[ENG]**
1. Set up the GPU usage environment in PyTorch.
2. Configure transformations that define data preprocessing and normalization for input images.
3. Define the dataset and apply the data transform.
4. Create an image classification model using the ResNet-34 architecture.
5. Apply L2 normalization.
6. K-Fold cross-validation learns the model and evaluates its performance.
7. Save the model weight if you have the highest accuracy per fold.
8. Save the learning and test results to a file.

<br>

**[KOR]**
1. PyTorch에서 GPU 사용 환경을 설정합니다.
2. 입력 이미지에 대한 데이터 전처리 및 정규화를 정의하는 변환을 구성합니다.
3. 데이터셋을 정의하고, 데이터 변환을 적용합니다.
4. ResNet-34 아키텍처를 사용하여 이미지 분류 모델을 생성합니다.
5. L2 정규화를 적용합니다.
6. K-Fold 교차 검증을 통해 모델을 학습하고 성능을 평가합니다.
7. 폴드별 최고 정확도를 가진 경우 모델 가중치를 저장합니다.
8. 학습 및 테스트 결과를 파일에 저장합니다.

### ✔️ CSD-Server
Details in [CSD-Server Repository](https://github.com/CAP-JJANG/CSD-Server)  

### ✔️ WriteNow
Details in [WriteNow Repository](https://github.com/CAP-JJANG/WriteNow)  

### ✔️ RightNow
Details in [RightNow Repository](https://github.com/CAP-JJANG/RightNow)  


<br><br>
## 🦾 주요 기술
### CSD-Model
**Model - CNN**
* PyCharm IDE
* Python 3.9.13
* Scikit_learn 1.3.1
* Torch 1.13.1
* Torchvision 0.14.1

### CSD-Server
**Server - Django**
* PyCharm: IDE
* Python: 3.9.13
* Django: 4.2.5
* Djangorestframework: 3.14.0
* Librosa: 0.10.1
* Matplotlib: 3.7.2
* Numpy: 1.25.2
* Pillow: 10.0.1
* Pydub: 0.25.1
* Torch: 1.13.1
* Torchvision: 0.14.1

### RightNow
**Mobile - Android**
* Android Studio: Giraffe | 2022.3.1
* Gradle plugin: 8.1.1
* JDK: jbr-17
* Min SDK: 24
* Target SDK: 33
* Navigation : 2.7.3
* Retrofit: 2.9.0

### WriteNow
**Mobile - Android**
* Android Studio: Giraffe | 2022.3.1
* Gradle plugin: 8.1.1
* JDK: jbr-17
* Min SDK: 30
* Retrofit: 2.9.0
* Livedata: 2.6.2


<br><br>
## 🧬 모델 아키텍처
<div align="center">
  <img width="60%" alt="image" src="https://github.com/CAP-JJANG/.github/assets/92065911/7fcd5810-2541-4a52-a0aa-a758c61e8fc8">
</div>

<br><br>
## 🔗 서비스 아키텍처
<div align="center">
  <img width="80%" alt="image" src="https://github.com/CAP-JJANG/.github/assets/92065911/1b391640-4450-4db0-a662-e403e101600a">
</div>

<br><br>
## 👀 실행 화면
* **Watch** :watch:
  <img width="100%" alt="image" src="https://github.com/Capjjang23/.github/assets/92065911/08fb70df-859d-4dca-ad84-6ca1ef4a6520">
  <br>
* **Phone** :iphone:
  <img width="100%" alt="image" src="https://github.com/Capjjang23/.github/assets/92065911/3acf80b7-fa8d-4b8d-aadf-1066ba7d8a7f">

<br><br>
## 🤝 커밋 룰

| 커밋 구분 | 설명 |
| --- | --- |
| FEAT | (Feature) 개선 또는 기능 추가 |
| BUG | (Bug Fix) 버그 수정 |
| DOC | (Documentation) 문서 작업 |
| TST | (Test) 테스트 추가/수정 |
| BLD | (Build) 빌드 프로세스 관련 수정(yml) |
| PERF | (Performance) 속도 개선 |
| CLN | (Cleanup) 코드 정리/리팩토링 |

- `커밋 구분/~한다`는 명령어로 시작하여 한 눈에 어떤 작업을 했는지 알기 쉽게 적는다.
- 예시  
  `FEAT/Create~`, `FEAT/Add~`, `BUG/Fix~`, `DOC/Delete~`


<br><br>
## 👻 팀원
<table>
  <tr> 
    <td><a href="https://github.com/Ga-Long"><img src="https://avatars.githubusercontent.com/u/100428958?v=4" style="width:150%; height:150%;"></a></td>
    <td><a href="https://github.com/kimgwon"><img src="https://avatars.githubusercontent.com/u/92065911?v=4"></a></td>
    <td><a href="https://github.com/youngchive"><img src="https://avatars.githubusercontent.com/u/102659915?v=4"></a></td>
    <td><a href="https://github.com/mmihye"><img src="https://avatars.githubusercontent.com/u/92644651?v=4"></a></td>
  </tr>
  <tr> 
    <td align='center'><strong>이가현</strong></td> 
    <td align='center'><strong>김지원</strong></td> 
    <td align='center'><strong>조서영</strong></td> 
    <td align='center'><strong>석미혜</strong></td> 
  </tr>
</table>

<br><br>
## 🤖 라이센스
* MakeDataset : Apache-2.0 license
* CSD-Model : Apache-2.0 license
* CSD-Server : Apache-2.0 license
* RightNow : Apache-2.0 license
* WriteNow : Apache-2.0 license
